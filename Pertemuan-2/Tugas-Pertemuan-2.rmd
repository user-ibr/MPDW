```{r install-packages, eval=FALSE}
pkgs <- c(
  "forecast", "graphics", "TTR", "TSA", "aTSA", "lmtest", "MLmetrics", 
  "car", "dplyr", "MASS", "tsibble", "tseries", "ggplot2", "nortest", "readxl"
)

for (p in pkgs) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p)
  }
}
```

```{r load-packages, message=FALSE, warning=FALSE}
library(forecast)
library(graphics)
library(TTR)
library(TSA)
library(aTSA)
library(lmtest)
library(MLmetrics)
library(car)
library(dplyr)
library(MASS)
library(tsibble)
library(tseries)
library(ggplot2)
library(nortest)
library(readxl)
```
# Single Moving Average
```{r}
data_excel <- read_excel("C:/Users/De Ibrahim/kuliah/sem 5/Book1.xlsx")
data.ts <- ts(data_excel$Price, frequency = 12, start = c(2015,1))
n <- length(data.ts)
n_train <- floor(0.8 * n)
train_ma.ts <- data.ts[1:n_train]
test_ma.ts  <- data.ts[(n_train+1):n]
m <- 12 
data.sma <- SMA(train_ma.ts, n = m)

data.ramal <- c(NA, data.sma)

data.gab <- cbind(
  aktual    = c(data.ts),
  pemulusan = c(data.sma, rep(NA, n - length(data.sma))),
  ramalan   = c(data.ramal, rep(data.ramal[length(data.ramal)], n - length(data.ramal)))
)

ts.plot(data.ts, xlab="Periode", ylab="Price", main=paste("SMA N =", m))
points(data.ts)
lines(data.gab[,2], col="green", lwd=2)
lines(data.gab[,3], col="red", lwd=2)
legend("topleft",
       c("Data Aktual","Pemulusan","Peramalan"),
       lty=1, col=c("black","green","red"), cex=0.7)

error_train.sma <- train_ma.ts - data.ramal[1:length(train_ma.ts)]

SSE_train.sma <- sum(error_train.sma[(m+1):length(train_ma.ts)]^2)
MSE_train.sma <- mean(error_train.sma[(m+1):length(train_ma.ts)]^2)
MAPE_train.sma <- mean(abs(error_train.sma[(m+1):length(train_ma.ts)] / 
                           train_ma.ts[(m+1):length(train_ma.ts)])*100)

akurasi_train.sma <- matrix(c(SSE_train.sma, MSE_train.sma, MAPE_train.sma))
row.names(akurasi_train.sma) <- c("SSE","MSE","MAPE")
colnames(akurasi_train.sma) <- paste("Akurasi m =", m)
akurasi_train.sma

error_test.sma <- test_ma.ts - data.gab[(n_train+1):n, 3]

SSE_test.sma <- sum(error_test.sma^2)
MSE_test.sma <- mean(error_test.sma^2)
MAPE_test.sma <- mean(abs(error_test.sma/test_ma.ts*100))

akurasi_test.sma <- matrix(c(SSE_test.sma, MSE_test.sma, MAPE_test.sma))
row.names(akurasi_test.sma) <- c("SSE","MSE","MAPE")
colnames(akurasi_test.sma) <- paste("Akurasi m =", m)
akurasi_test.sma
```
Model 1 (MAPE â‰ˆ 19,26%)

âž¡ï¸ Akurasi ramalan tergolong cukup bagus, tapi masih ada selisih sekitar 19% rata-rata dari nilai asli. Jadi masih bisa dipakai, tapi prediksinya belum terlalu presisi.

Model 2 (MAPE â‰ˆ 11,31%)

âž¡ï¸ Akurasi ramalan lebih baik dibanding model pertama, dengan rata-rata kesalahan hanya sekitar 11% dari nilai asli. Ini sudah masuk kategori cukup akurat dan lebih layak dipakai.

ðŸ‘‰ Kesimpulan singkat: Model kedua lebih akurat dibanding model pertama, karena nilai MAPE dan MSE lebih kecil.

# Double Moving Average
```{r}
data.ts <- ts(data_excel$Price, frequency = 12, start = c(2015,1))

n <- length(data.ts)
n_train <- floor(0.8 * n)
train_ma.ts <- data.ts[1:n_train]
test_ma.ts  <- data.ts[(n_train+1):n]

m <- 12 
data.sma <- SMA(train_ma.ts, n = m)     
dma <- SMA(data.sma, n = m)            

At <- 2*data.sma - dma
Bt <- 2/(m-1) * (data.sma - dma)

data.dma <- At + Bt
data.ramal2 <- c(NA, data.dma)
t <- 1:length(test_ma.ts)
f <- c()
for (i in t) {
  f[i] <- At[length(At)] + Bt[length(Bt)]*i
}

data.gab2 <- cbind(
  aktual     = c(data.ts),
  pemulusan1 = c(data.sma, rep(NA, n - length(data.sma))),
  pemulusan2 = c(dma, rep(NA, n - length(dma))),
  At         = c(At, rep(NA, n - length(At))),
  Bt         = c(Bt, rep(NA, n - length(Bt))),
  ramalan    = c(data.ramal2, f)
)
ts.plot(data.ts, xlab="Periode", ylab="Price", main=paste("DMA N =", m, "Data Price"))
points(data.ts)
lines(data.gab2[,3], col="green", lwd=2)  # pemulusan 2
lines(data.gab2[,6], col="red", lwd=2)    # ramalan
legend("topleft", c("Data Aktual","Pemulusan","Peramalan"),
       lty=1, col=c("black","green","red"), cex=0.7)

error_train.dma <- train_ma.ts - data.ramal2[1:length(train_ma.ts)]

SSE_train.dma <- sum(error_train.dma[(2*m):length(train_ma.ts)]^2)
MSE_train.dma <- mean(error_train.dma[(2*m):length(train_ma.ts)]^2)
MAPE_train.dma <- mean(abs(error_train.dma[(2*m):length(train_ma.ts)] /
                           train_ma.ts[(2*m):length(train_ma.ts)])*100)

akurasi_train.dma <- matrix(c(SSE_train.dma, MSE_train.dma, MAPE_train.dma))
row.names(akurasi_train.dma) <- c("SSE","MSE","MAPE")
colnames(akurasi_train.dma) <- paste("Akurasi m =", m)
akurasi_train.dma

error_test.dma <- test_ma.ts - data.gab2[(n_train+1):n,6]

SSE_test.dma <- sum(error_test.dma^2)
MSE_test.dma <- mean(error_test.dma^2)
MAPE_test.dma <- mean(abs(error_test.dma/test_ma.ts*100))

akurasi_test.dma <- matrix(c(SSE_test.dma, MSE_test.dma, MAPE_test.dma))
row.names(akurasi_test.dma) <- c("SSE","MSE","MAPE")
colnames(akurasi_test.dma) <- paste("Akurasi m =", m)
akurasi_test.dma
```
Model 1 (MAPE â‰ˆ 18,88%)

âž¡ï¸ Ramalan tergolong cukup bagus, dengan rata-rata kesalahan sekitar 19% dari nilai asli. Masih bisa dipakai, meski belum sangat presisi.

Model 2 (MAPE â‰ˆ 30,63%)

âž¡ï¸ Akurasi ramalan kurang baik, karena rata-rata kesalahan mencapai 30%, artinya model ini jauh dari nilai sebenarnya dan kurang layak dijadikan acuan.

ðŸ‘‰ Kesimpulan singkat: Model pertama lebih akurat dan layak dipakai, sedangkan model kedua memiliki tingkat kesalahan yang terlalu besar.

# Single Exponential Smoothing
```{r}
data.ts <- ts(data_excel$Price, frequency = 12, start = c(2015,1))

n <- length(data.ts)
n_train <- floor(0.8 * n)

train.ts <- ts(data.ts[1:n_train], frequency = 12, start = c(2015,1))
test.ts  <- ts(data.ts[(n_train+1):n], frequency = 12,
               start = c(2015, 1 + n_train/12))

ses1 <- HoltWinters(train.ts, gamma=FALSE, beta=FALSE, alpha=0.2)
ramalan1 <- predict(ses1, n.ahead=length(test.ts))

ses2 <- HoltWinters(train.ts, gamma=FALSE, beta=FALSE, alpha=0.7)
ramalan2 <- predict(ses2, n.ahead=length(test.ts))

HWopt <- HoltWinters(train.ts, gamma=FALSE, beta=FALSE, alpha=NULL)
ramalanopt <- predict(HWopt, n.ahead=length(test.ts))

SSE1 <- ses1$SSE
MSE1 <- SSE1/length(train.ts)
RMSE1 <- sqrt(MSE1)

akurasi1 <- matrix(c(SSE1,MSE1,RMSE1),
                   ncol=1, dimnames=list(c("SSE","MSE","RMSE"),
                                         "Alpha=0.2"))
akurasi1

SSE2 <- ses2$SSE
MSE2 <- SSE2/length(train.ts)
RMSE2 <- sqrt(MSE2)

akurasi2 <- matrix(c(SSE2,MSE2,RMSE2),
                   ncol=1, dimnames=list(c("SSE","MSE","RMSE"),
                                         "Alpha=0.7"))
akurasi2

ramalan1 <- as.numeric(ramalan1)
ramalan2 <- as.numeric(ramalan2)
ramalanopt <- as.numeric(ramalanopt)

e1   <- test.ts - ramalan1
e2   <- test.ts - ramalan2
eopt <- test.ts - ramalanopt

SSEtesting1  <- sum(e1^2,  na.rm = TRUE)
MSEtesting1  <- mean(e1^2, na.rm = TRUE)
RMSEtesting1 <- sqrt(MSEtesting1)

SSEtesting2  <- sum(e2^2,  na.rm = TRUE)
MSEtesting2  <- mean(e2^2, na.rm = TRUE)
RMSEtesting2 <- sqrt(MSEtesting2)

SSEtestingopt  <- sum(eopt^2,  na.rm = TRUE)
MSEtestingopt  <- mean(eopt^2, na.rm = TRUE)
RMSEtestingopt <- sqrt(MSEtestingopt)

akurasitesting_SSE <- matrix(c(SSEtesting1, SSEtesting2, SSEtestingopt),
                             nrow = 3,
                             dimnames = list(c("SSE1","SSE2","SSEopt"), "Nilai"))

akurasitesting_MSE <- matrix(c(MSEtesting1, MSEtesting2, MSEtestingopt),
                             nrow = 3,
                             dimnames = list(c("MSE1","MSE2","MSEopt"), "Nilai"))

akurasitesting_RMSE <- matrix(c(RMSEtesting1, RMSEtesting2, RMSEtestingopt),
                              nrow = 3,
                              dimnames = list(c("RMSE1","RMSE2","RMSEopt"), "Nilai"))

akurasitesting_SSE
akurasitesting_MSE
akurasitesting_RMSE

MAPE1 <- mean(abs(e1/test.ts))*100
MAPE2 <- mean(abs(e2/test.ts))*100
MAPEopt <- mean(abs(eopt/test.ts))*100

akurasi_MAPE <- data.frame(
  Model = c("Alpha=0.2","Alpha=0.7","Alpha=opt"),
  MAPE = c(MAPE1,MAPE2,MAPEopt)
)
akurasi_MAPE
```
Î± = 0.2 â†’ SSE, MSE, RMSE tinggi â†’ model lambat menyesuaikan perubahan terbaru pada data, smoothing lebih halus tapi kurang responsif.

Î± = 0.7 â†’ SSE, MSE, RMSE jauh lebih rendah â†’ model lebih responsif terhadap fluktuasi harga, prediksi lebih akurat di training.

Î± rendah (0.2) â†’ terlalu lambat, kurang responsif.

Î± tinggi (0.7) atau Î± optimal â†’ lebih responsif, menghasilkan prediksi lebih akurat pada testing, terlihat dari MAPE lebih rendah.

Pada data testing, semua SES relatif stabil, tetapi model dengan Î± tinggi menangkap perubahan lebih baik.

# Double Exponential Smoothing & Hotwinters
```{r}
library(forecast)
data1 <- read_excel("C:/Users/De Ibrahim/kuliah/sem 5/Book1.xlsx")

n <- nrow(data1)
n_train <- floor(0.8 * n)

training <- data1[1:n_train, ]
testing  <- data1[(n_train+1):n, ]

train.ts <- ts(training$Price, frequency = 12, 
               start = c(as.numeric(format(training$Date[1], "%Y")), 
                         as.numeric(format(training$Date[1], "%m"))))

des.1 <- HoltWinters(train.ts, alpha = 0.2, beta = 0.2, gamma = FALSE)
plot(des.1, main="DES1 (Î±=0.2, Î²=0.2)")

des.2 <- HoltWinters(train.ts, alpha = 0.6, beta = 0.3, gamma = FALSE)
plot(des.2, main="DES2 (Î±=0.6, Î²=0.3)")

des.opt <- HoltWinters(train.ts, gamma = FALSE)
plot(des.opt, main="DES Optimum (Î± & Î² otomatis)")

h <- nrow(testing)

ramalandes1_mean <- predict(des.1, n.ahead = h)
ramalandes2_mean <- predict(des.2, n.ahead = h)
ramalandesopt_mean <- predict(des.opt, n.ahead = h)

ssedes.train1 <- des.1$SSE
msedes.train1 <- ssedes.train1 / length(train.ts)
sisaandes1 <- residuals(des.1)
mapedes.train1 <- mean(abs(sisaandes1 / train.ts) * 100, na.rm=TRUE)

akurasides.1 <- matrix(c(ssedes.train1, msedes.train1, mapedes.train1),
                       nrow=3, dimnames=list(c("SSE","MSE","MAPE"),
                                             "DES(Î±=0.2,Î²=0.2)"))
akurasides.1

ssedes.train2 <- des.2$SSE
msedes.train2 <- ssedes.train2 / length(train.ts)
sisaandes2 <- residuals(des.2)
mapedes.train2 <- mean(abs(sisaandes2 / train.ts) * 100, na.rm=TRUE)

akurasides.2 <- matrix(c(ssedes.train2, msedes.train2, mapedes.train2),
                       nrow=3, dimnames=list(c("SSE","MSE","MAPE"),
                                             "DES(Î±=0.6,Î²=0.3)"))
akurasides.2

ssedes.trainopt <- des.opt$SSE
msedes.trainopt <- ssedes.trainopt / length(train.ts)
sisaandesopt <- residuals(des.opt)
mapedes.trainopt <- mean(abs(sisaandesopt / train.ts) * 100, na.rm=TRUE)

akurasides.opt <- matrix(c(ssedes.trainopt, msedes.trainopt, mapedes.trainopt),
                         nrow=3, dimnames=list(c("SSE","MSE","MAPE"),
                                               "DES Optimum"))
akurasides.opt

selisihdes1 <- ramalandes1_mean - testing$Price
SSEtestingdes1 <- sum(selisihdes1^2)
MSEtestingdes1 <- mean(selisihdes1^2)
MAPEtestingdes1 <- mean(abs(selisihdes1 / testing$Price) * 100)

selisihdes2 <- ramalandes2_mean - testing$Price
SSEtestingdes2 <- sum(selisihdes2^2)
MSEtestingdes2 <- mean(selisihdes2^2)
MAPEtestingdes2 <- mean(abs(selisihdes2 / testing$Price) * 100)

selisihdesopt <- ramalandesopt_mean - testing$Price
SSEtestingdesopt <- sum(selisihdesopt^2)
MSEtestingdesopt <- mean(selisihdesopt^2)
MAPEtestingdesopt <- mean(abs(selisihdesopt / testing$Price) * 100)

akurasitestingdes <- matrix(c(SSEtestingdes1, MSEtestingdes1, MAPEtestingdes1,
                              SSEtestingdes2, MSEtestingdes2, MAPEtestingdes2,
                              SSEtestingdesopt, MSEtestingdesopt, MAPEtestingdesopt),
                            nrow=3, ncol=3,
                            dimnames=list(c("SSE","MSE","MAPE"),
                                          c("DES1 (0.2,0.2)",
                                            "DES2 (0.6,0.3)",
                                            "DES Optimum")))
akurasitestingdes

plot(train.ts, xlim=c(1, n), ylim=range(c(train.ts, ramalandesopt_mean, ramalandes1_mean, ramalandes2_mean)),
     main="Double Exponential Smoothing (DES) - Prediksi Testing",
     ylab="Price", xlab="Time")
lines(n_train + 1:h, ramalandes1_mean, col="blue", lty=2)
lines(n_train + 1:h, ramalandes2_mean, col="red", lty=2)
lines(n_train + 1:h, ramalandesopt_mean, col="green", lty=2)
legend("topleft", legend=c("DES1 (0.2,0.2)", "DES2 (0.6,0.3)", "DES Optimum"),
       col=c("blue","red","green"), lty=2, bty="n")
```
Model Holtâ€“Winters menunjukkan tren naik yang stabil, tetapi dalam uji perbandingan, DES Optimum memberikan hasil prediksi paling akurat dengan error terkendali (MAPE â‰ˆ 13,8%), sehingga layak dijadikan model utama untuk forecasting.

# Perbandingan SES vs DES (MSE Test)
```{r}
MSEfull <- matrix(c(MSEtesting1, MSEtesting2, MSEtestingopt,
                    MSEtestingdes1, MSEtestingdes2, MSEtestingdesopt),
                  nrow=3, ncol=2,
                  dimnames=list(c("Skenario 1","Skenario 2","Optimum"),
                                c("SES","DES")))
MSEfull
```
Interpretasi Hasil Perbandingan SES dan DES

Single Exponential Smoothing (SES) â†’ hasilnya muncul NaN, artinya metode ini tidak cocok untuk data kamu (mungkin karena pola tren yang cukup kuat, sedangkan SES hanya bisa mengikuti data yang stasioner tanpa tren). Jadi wajar kalau tidak menghasilkan ukuran akurasi yang valid.

Double Exponential Smoothing (DES) â†’ jauh lebih baik, karena DES bisa menangkap tren.

Pada parameter (Î±=0.2, Î²=0.2) â†’ MAPE 56% â†’ akurasinya masih rendah (prediksi jauh dari data aktual).

Pada parameter (Î±=0.6, Î²=0.3) â†’ MAPE turun ke 40% â†’ ada perbaikan, tetapi masih cukup meleset.

DES Optimum â†’ MAPE hanya 13.84% â†’ ini sudah termasuk akurat (kategori baik untuk forecasting). SSE dan MSE juga jauh lebih kecil dibanding skenario awal.

SMA bisa lebih akurat dari SES (MAPE 11,31% vs 16%), tetapi masih kalah stabil dibanding DES yang memang dirancang untuk data dengan tren.

ðŸ‘‰ Kesimpulan:

SES memiliki error tinggi, SMA bisa disaingingi DES tapi kalah stabil, Holt Winters tidak cocok karena bukan data musiman, DMA memiliki MAPE terlalu tinggi (30%), DES tertentu seperti Î±=0.2, Î²=0.2 atau Î±=0.6, Î²=0.3 masih kurang bagus karena MAPE di atas 40%

DES optimum adalah pilihan terbaik, karena mampu menurunkan error secara signifikan.

Dengan MAPE sekitar 13.8%, hasil prediksi bisa dianggap cukup andal untuk analisis dan pengambilan keputusan, meskipun belum sampai kategori "sangat baik" (<10%)

